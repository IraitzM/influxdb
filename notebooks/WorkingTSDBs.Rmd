---
title: "Working with TSDBs"
output: html_notebook
---

Along this lines we will familiarize ourselves with time series data bases, in this case InfluxDB.

![](images/Influxdb_logo.png)

For this code to run a local installation of it is assumed to be running and listening on port 8086. Docker can be easily used for this purpose, just pulling the image and making it run.
```
docker run -d --name=some-influx \
 -p 8086:8086 \
 -v  /tmp/testdata/influx:/root/.influxdbv2 quay.io/influxdb/influxdb:v2.0.3
```
We are good to go. Now, from R there is a library we may use to interact with our database.

```{r}
library(dplyr)
library(magrittr)
library(lubridate)
library(influxdbclient)
```

```{r}
# Opens connection to InfluxDb
client <- InfluxDBClient$new(url = "http://localhost:8086", token="uEDVlXcEYgPnyjkfM1Rlh7TnWEsiXYoJkcpvmFuOUQJU_mRublorZ4OmoAsh9tlPWU5ZQ41TmGhKmXekPYwDUQ==", org="Deusto")
```


Done! We can interrogate our database about its status.

```{r}
# Ready status
client$ready()
```
```{r}
# Healt info
client$health()
```
And check if there is any bucket that can be used.


```{r}
client$query('from(bucket:"Santander") |> range(start: -1s)')
```

In order to test InfluxDB's functionalities we will need some sensor data. We can retrieve some data from Santander's open platform for example.

![](images/sod.png)
They have traffic sensors all over the city informed every minute or so so it should be a good candidate for sensor metrics. You can fin more information [here](http://datos.santander.es/resource/?ds=datos-trafico&id=626c514f-d6a0-4efe-95bd-3f1cd4dc531e&ft=CSV).

```{r}
url <- "http://datos.santander.es/api/rest/datasets/mediciones.csv?items=482&rnd=863974585"
data <- read.csv(url)
```

Nice, lets have a look at it.

```{r}
data
```
We will do some cleansing. URL and identifier don't seem very useful and idSensor and medida have the same value.

```{r}
data %<>% 
  select(dc.modified, ayto.medida, ayto.intensidad, ayto.ocupacion, ayto.carga) %>% 
  rename(timestamp = dc.modified, sensorid = ayto.medida, intensity = ayto.intensidad, occupancy = ayto.ocupacion, load = ayto.carga ) %>%
  mutate(timestamp = as_datetime(timestamp, tz = "UTC"))

# Lets add the measurement (table) it belongs to
data[,"_measurement"] <- "traffic"

# And check the output
data
```
Great now that we have our data ready, let's put it into the database. Some things to clarify:

* *bucket*: name of the database where our data will be stored
* *precision*: to be used when timestamp is parsed
* *tags*: tags are how we are going to identify metric types or device identifiers in order to group them afterwards
* *fields*: which columns from our dataframe contain measured values (sensor data)
* *time*: timestamp columns within our dataframe

```{r}
client$write(data, 
             bucket = "Santander", 
             precision = "ms",
             tagCols = c("sensorid"),
             fieldCols = c("intensity", "occupancy","load"),
             timeCol = "timestamp")
```

Great! We have our database informed. Have we? Let's check it out. We need to muster Flux for that...
```
from(bucket: "Santander") 
  |> range(start: -5h) 
  |> filter(fn: (r) => r["_measurement"] == "traffic" and r["_field"] == "load") 
  |> group(columns: ["sensorid"]) 
  |> yield(name: "mean")
```

```{r}
q <- 'from(bucket: "Santander") |> range(start: -7h) |> filter(fn: (r) => r["_measurement"] == "traffic" and r["_field"] == "load")  |> group(columns: ["sensorid"]) |> yield(name: "mean")'
result <- client$query(q, flatSingleResult = TRUE)
length(result)
```

That's our requested information for all 482 sensor ids. Let's check the first one.

```{r}
result[1]
```


So each measurement will be tagged by a sensor id, given that this are independent from one another. We could ask for information not existent in our database as time series might be required to interpolate their information in order to e useful...
```
from(bucket: "Santander") 
  |> range(start: -5h, stop: -4h)
  |> filter(fn: (r) => r["_measurement"] == "traffic")
  |> filter(fn: (r) => r["_field"] == "load") 
  |> group(columns: ["sensorid"]) 
  |> aggregateWindow(every: 10s, fn: mean, createEmpty: true)
  |> yield(name: "mean")
```


```{r}
q <- 'from(bucket: "Santander") |> range(start: -5h, stop: -4h) |> filter(fn: (r) => r["_measurement"] == "traffic")|> filter(fn: (r) => r["_field"] == "load") |> group(columns: ["sensorid"]) |> aggregateWindow(every: 10m, fn: mean, createEmpty: true)|> yield(name: "mean")'
result <- client$query(q)
length(result)
```

```{r}
result[1]
```

We can keep on adding information. We can define a function that does it for us.

```{r}
addinfo <- function()
{ 
  # Get the information
  url <- "http://datos.santander.es/api/rest/datasets/mediciones.csv?items=482&rnd=863974585"
  data <- read.csv(url)
  # Format and clean
  data %<>% select(dc.modified, ayto.medida, ayto.intensidad, ayto.ocupacion, ayto.carga)%>% rename(timestamp = dc.modified, sensorid = ayto.medida, intensity = ayto.intensidad, occupancy = ayto.ocupacion, load = ayto.carga )%>% mutate(timestamp = as_datetime(timestamp, tz = "UTC"))
  # Insert
  client$write(data, 
             bucket = "Santander", 
             precision = "ms",
             tagCols = c("sensorid"),
             fieldCols = c("intensity", "occupancy","load"),
             timeCol = "timestamp")
}
```

And keep calling as much as we want.

```{r}
addinfo()
Sys.sleep(90) # Wait for 90 seconds
addinfo()
Sys.sleep(90) # Wait for 90 seconds
addinfo()
Sys.sleep(90) # Wait for 90 seconds
addinfo()
```

Let's see what we have got.

```{r}
r <- influx_query(con, db = "test", query = "SELECT time, intensity FROM traffic WHERE time > now() -1h AND sensorid = '1001'")
as.data.frame(r)
```

Nice, two measurements. But being a time series we do know that we will need some intermediate measurements. That is when a Time Series Database becomes handy.

```{r}
r <- influx_query(con, db = "test", query = "SELECT time, mean(intensity) as intensity, mean(load) as load FROM traffic WHERE time > now() -10m AND sensorid = '1001' GROUP BY TIME(2m)")
as.data.frame(r)
```
Wait, timestamps have been rounded up and data averaged.

So we can ask for information at a given point in time even if it doesn't exist? Yes, but there is no data for those "artificially created" points in time. Some interpolation might be required.

```{r}
r <- influx_query(con, db = "test", query = "SELECT time, mean(intensity) as intensity, mean(load) as load FROM traffic WHERE time > now() -10m AND sensorid = '1001' GROUP BY TIME(1m) FILL(linear)")
as.data.frame(r)
```

Nice! That's cool. Extrapolating can be a little bit too much but at least those intermediate values are informed now.

Another interesting aspect are retention policies. Do we need same data resolution for all our measurements? The idea is that storage can be compromised when storing sensor information for long periods and old information might not be as relevant as recent information, so, can we store an aggregated version of it?

```{r}
show_retention_policies(con, db="test")
```
Let's erase old information.

```{r}
create_retention_policy(con, db = "test", rp_name = "default_rp", duration = "1h", replication = 1, default = TRUE)
```

```{r}
show_retention_policies(con, db="test")
```

That means every bit of information will expire when it is 1 hour old. Well, we can prevent this by calling what it is called a continuous query. This query will trigger every defined time lapse and aggregate the information into a separate measurement.

```{r}
q <- "CREATE CONTINUOUS QUERY traffic10m ON test BEGIN
  SELECT mean(intensity) AS intensity, mean(load) AS load
  INTO default_rp.downsampled_traffic
  FROM traffic
  GROUP BY time(10m), sensorid
END"
influx_query(con, db = "test", query = q)
```

Let's add some more info.
```{r}
addinfo()
Sys.sleep(90) # Wait for 90 seconds
addinfo()
Sys.sleep(90) # Wait for 90 seconds
addinfo()
Sys.sleep(90) # Wait for 90 seconds
addinfo()
```

That means that if we wait enough...
```{r}
r <- influx_query(con, db = "test", query = "SELECT time, mean(intensity) as intensity, mean(load) as load FROM traffic WHERE time > now() -10m AND sensorid = '1001' GROUP BY TIME(1m) FILL(linear)")
as.data.frame(r)
```

Let's see what we have...
```{r}
show_measurements(con, db="test", where = NULL)
```

```{r}
r <- influx_query(con, db = "test", query = "SELECT * FROM downsampled_traffic WHERE sensorid = '1001'")
as.data.frame(r)
```

Let's clear everything up by calling
```
docker stop some-influx
```
