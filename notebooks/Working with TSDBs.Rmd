---
title: "Working with TSDBs"
output: html_notebook
---

Along this lines we will familiarize ourselves with time series data bases, in this case InfluxDB.

![](images/Influxdb_logo.png)

For this code to run a local installation of it is assumed to be running and listening on port 8086. Docker can be easily used for this purpose, just pulling the image.

```{bash}
docker pull influxdb
```
And making it run.

```{bash}
docker run -p 8086:8086 -p 2003:2003 -e INFLUXDB_GRAPHITE_ENABLED=true -v $PWD:/var/lib/influxdb --name some-influx -d influxdb
```
We are good to go. Now, from R there is a library we may use to interact with our database.

```{r}
library(influxdbr)
# Opens connection to InfluxDb
con <- influx_connection(host = "localhost", port = 8086)
```

Done! We can interrogate our database about existing databases.

```{r}
show_databases(con)
```
There is none. Ok, lets create one for our tests.

```{r}
# create new database
create_database(con = con, db = "test")
show_databases(con)
```
In order to test InfluxDB's functionalities we will need some sensor data. We can retrieve some data from Santander's open platform for example.

![](images/sod.png)
They have traffic sensors all over the city informed every minute or so so it should be a good candidate for sensor metrics. You can fin more information [here](http://datos.santander.es/resource/?ds=datos-trafico&id=626c514f-d6a0-4efe-95bd-3f1cd4dc531e&ft=CSV).

```{r}
url <- "http://datos.santander.es/api/rest/datasets/mediciones.csv?items=482&rnd=863974585"
data <- read.csv(url)
```

Nice, lets have a look at it.

```{r}
data
```
We will do some cleansing. URL and identifier don't seem very useful and idSensor and medida have the same value.

```{r}
library(dplyr)
library(magrittr)
library(lubridate)
data %<>% select(dc.modified, ayto.medida, ayto.intensidad, ayto.ocupacion, ayto.carga) %>% rename(timestamp = dc.modified, sensorid = ayto.medida, intensity = ayto.intensidad, occupancy = ayto.ocupacion, load = ayto.carga ) %>% mutate(timestamp = as_datetime(timestamp, tz = "UTC"))
data
```
Great now that we have our data ready, let's put it into the database.

```{r}
influx_write(con = con, 
             db = "test",
             x = data,
             time_col = "timestamp", tag_cols = c("sensorid"),
             measurement = "traffic")
```

Great! We have our database informed. Have we? Let's check it out. First, see if measurement exists.

```{r}
show_measurements(con, db="test", where = NULL)
```
So each measurement will be tagged by a sensorid, given that this are independent from one another.

```{r}
show_tag_keys(con, db="test", measurement = NULL)
```
```{r}
show_tag_values(con, db="test", measurement = NULL, key="sensorid")
```

And each sensorid should have three available fields.

```{r}
show_field_keys(con, db="test", measurement = NULL)
```

So we should be able to ask for the intensity of a given sensor at a given point in time.

```{r}
r <- influx_query(con, db = "test", query = "SELECT time, intensity FROM traffic WHERE time > now() -1h")
as.data.frame(r)
```

```{r}
r <- influx_query(con, db = "test", query = "SELECT time, intensity FROM traffic WHERE time > now() -1h AND sensorid = '1001'")
as.data.frame(r)
```

We can keep on adding information. We can define a function that does it for us.

```{r}
addinfo <- function()
{ 
  # Get the information
  url <- "http://datos.santander.es/api/rest/datasets/mediciones.csv?items=482&rnd=863974585"
  data <- read.csv(url)
  # Format and clean
  data %<>% select(dc.modified, ayto.medida, ayto.intensidad, ayto.ocupacion, ayto.carga)%>% rename(timestamp = dc.modified, sensorid = ayto.medida, intensity = ayto.intensidad, occupancy = ayto.ocupacion, load = ayto.carga )%>% mutate(timestamp = as_datetime(timestamp, tz = "UTC"))
  # Insert
  influx_write(con = con, 
               db = "test",
               x = data,
               time_col = "timestamp", tag_cols = c("sensorid"),
               measurement = "traffic")
}
```

And keep calling as much as we want.

```{r}
addinfo()
Sys.sleep(90) # Wait for 90 seconds
addinfo()
Sys.sleep(90) # Wait for 90 seconds
addinfo()
Sys.sleep(90) # Wait for 90 seconds
addinfo()
```

Let's see what we have got.

```{r}
r <- influx_query(con, db = "test", query = "SELECT time, intensity FROM traffic WHERE time > now() -1h AND sensorid = '1001'")
as.data.frame(r)
```

Nice, two measurements. But being a time series we do know that we will need some intermediate measurements. That is when a Time Series Database becomes handy.

```{r}
r <- influx_query(con, db = "test", query = "SELECT time, mean(intensity) as intensity, mean(load) as load FROM traffic WHERE time > now() -10m AND sensorid = '1001' GROUP BY TIME(2m)")
as.data.frame(r)
```
Wait, timestamps have been rounded up and data averaged.

So we can ask for information at a given point in time even if it doesn't exist? Yes, but there is no data for those "artificially created" points in time. Some interpolation might be required.

```{r}
r <- influx_query(con, db = "test", query = "SELECT time, mean(intensity) as intensity, mean(load) as load FROM traffic WHERE time > now() -10m AND sensorid = '1001' GROUP BY TIME(1m) FILL(linear)")
as.data.frame(r)
```

Nice! That's cool. Extrapolating can be a little bit too much but at least those intermediate values are informed now.

Another interesting aspect are retention policies. Do we need same data resolution for all our measurements? The idea is that storage can be compromised when storing sensor information for long periods and old information might not be as relevant as recent information, so, can we store an aggregated version of it?

```{r}
show_retention_policies(con, db="test")
```
Let's erase old information.

```{r}
create_retention_policy(con, db = "test", rp_name = "default_rp", duration = "1h", replication = 1, default = TRUE)
```

```{r}
show_retention_policies(con, db="test")
```

That means every bit of information will expire when it is 1 hour old. Well, we can prevent this by calling what it is called a continuous query. This query will trigger every defined time lapse and aggregate the information into a separate measurement.

```{r}
q <- "CREATE CONTINUOUS QUERY traffic10m ON test BEGIN
  SELECT mean(intensity) AS intensity, mean(load) AS load
  INTO default_rp.downsampled_traffic
  FROM traffic
  GROUP BY time(10m), sensorid
END"
influx_query(con, db = "test", query = q)
```

Let's add some more info.
```{r}
addinfo()
Sys.sleep(90) # Wait for 90 seconds
addinfo()
Sys.sleep(90) # Wait for 90 seconds
addinfo()
Sys.sleep(90) # Wait for 90 seconds
addinfo()
```

That means that if we wait enough...
```{r}
r <- influx_query(con, db = "test", query = "SELECT time, mean(intensity) as intensity, mean(load) as load FROM traffic WHERE time > now() -10m AND sensorid = '1001' GROUP BY TIME(1m) FILL(linear)")
as.data.frame(r)
```

Let's see what we have...
```{r}
show_measurements(con, db="test", where = NULL)
```

```{r}
r <- influx_query(con, db = "test", query = "SELECT * FROM downsampled_traffic WHERE sensorid = '1001'")
as.data.frame(r)
```

Let's clear everything up
```{bash}
docker stop some-influx
```
